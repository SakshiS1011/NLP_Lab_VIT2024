{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- **Name:** Sakshi Savardekar\n",
        "- **Roll No:** 22102A2002\n",
        "- **Class:** CMPN BE-A\n",
        "- **Subject:** NLP Lab\n",
        "- **Github Link:** https://github.com/SakshiS1011/NLP_Lab_VIT2024.git"
      ],
      "metadata": {
        "id": "6pOs7GR13vFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing - Experiment 1"
      ],
      "metadata": {
        "id": "rUsyRtu13vE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the libraries\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from collections import Counter\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import TextBlob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpADCAcn3__v",
        "outputId": "92b6ac1d-f96b-41f0-cf93-c90927ce9f0c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = '''\"ðŸŒ Check out thIs amazing Article on natural Language prOCeSsing (NLP) at https://example.com/Nlp-article. It covers Everything from tokenization to sentiment analysis! ðŸ“ In todAy's digital age, we often Encounter noisy and Unstructured text dAtA. To prEpare it for analysis or mOdeling, we Need to apply Several preprocessing steps. Let's consider An example sentence: 'I can't belIeve how Excited I am About this AI mOdel! ðŸ˜ƒðŸš€' This sentence Contains UppercAse letters, emojis, and frequent words like 'is', 'it', 'for', 'to'. These elements, Including emoticons and the URL 'https://example.com/Nlp-article', must be cleaned or removed to facilitate accurate NLP processing.\"\n",
        "\n",
        "This paragraph deliberately includes uppercase letters, incorrect spellings (like \"prOCeSsing\", \"mOdeling\"), frequent words (\"is\", \"it\", \"for\", \"to\"), words in small letters, a URL, stopwords (\"this\", \"for\", \"to\"), whitespace issues, emoticons (ðŸ˜ƒ), and emojis (ðŸš€) and numbers 1,2,3,4. These impurities would need to be addressed through preprocessing steps to ensure the text is ready for further natural language processing tasks.  '''"
      ],
      "metadata": {
        "id": "yJ8P_2Pj4YBB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XnzeaOL4aMT",
        "outputId": "2f6e51fa-1d18-4d39-c416-5d215080e562"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"ðŸŒ Check out thIs amazing Article on natural Language prOCeSsing (NLP) at https://example.com/Nlp-article. It covers Everything from tokenization to sentiment analysis! ðŸ“ In todAy's digital age, we often Encounter noisy and Unstructured text dAtA. To prEpare it for analysis or mOdeling, we Need to apply Several preprocessing steps. Let's consider An example sentence: 'I can't belIeve how Excited I am About this AI mOdel! ðŸ˜ƒðŸš€' This sentence Contains UppercAse letters, emojis, and frequent words like 'is', 'it', 'for', 'to'. These elements, Including emoticons and the URL 'https://example.com/Nlp-article', must be cleaned or removed to facilitate accurate NLP processing.\"\n",
            "\n",
            "This paragraph deliberately includes uppercase letters, incorrect spellings (like \"prOCeSsing\", \"mOdeling\"), frequent words (\"is\", \"it\", \"for\", \"to\"), words in small letters, a URL, stopwords (\"this\", \"for\", \"to\"), whitespace issues, emoticons (ðŸ˜ƒ), and emojis (ðŸš€) and numbers 1,2,3,4. These impurities would need to be addressed through preprocessing steps to ensure the text is ready for further natural language processing tasks.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Lower Casing"
      ],
      "metadata": {
        "id": "UHrmqnVp4h57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = input_str.lower()\n",
        "print(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBChm8TK4j5X",
        "outputId": "99a056a5-f53f-4fde-bf4f-a2757dde794d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"ðŸŒ check out this amazing article on natural language processing (nlp) at https://example.com/nlp-article. it covers everything from tokenization to sentiment analysis! ðŸ“ in today's digital age, we often encounter noisy and unstructured text data. to prepare it for analysis or modeling, we need to apply several preprocessing steps. let's consider an example sentence: 'i can't believe how excited i am about this ai model! ðŸ˜ƒðŸš€' this sentence contains uppercase letters, emojis, and frequent words like 'is', 'it', 'for', 'to'. these elements, including emoticons and the url 'https://example.com/nlp-article', must be cleaned or removed to facilitate accurate nlp processing.\"\n",
            "\n",
            "this paragraph deliberately includes uppercase letters, incorrect spellings (like \"processing\", \"modeling\"), frequent words (\"is\", \"it\", \"for\", \"to\"), words in small letters, a url, stopwords (\"this\", \"for\", \"to\"), whitespace issues, emoticons (ðŸ˜ƒ), and emojis (ðŸš€) and numbers 1,2,3,4. these impurities would need to be addressed through preprocessing steps to ensure the text is ready for further natural language processing tasks.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Removal of URLs"
      ],
      "metadata": {
        "id": "B28ei-DY4p3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "input_str = url_pattern.sub('', input_str)\n",
        "print(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV6l-0MS4s8V",
        "outputId": "5955c4d8-c8ca-4c0f-ccdf-9b1b2c7d4603"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"ðŸŒ check out this amazing article on natural language processing (nlp) at  it covers everything from tokenization to sentiment analysis! ðŸ“ in today's digital age, we often encounter noisy and unstructured text data. to prepare it for analysis or modeling, we need to apply several preprocessing steps. let's consider an example sentence: 'i can't believe how excited i am about this ai model! ðŸ˜ƒðŸš€' this sentence contains uppercase letters, emojis, and frequent words like 'is', 'it', 'for', 'to'. these elements, including emoticons and the url ' must be cleaned or removed to facilitate accurate nlp processing.\"\n",
            "\n",
            "this paragraph deliberately includes uppercase letters, incorrect spellings (like \"processing\", \"modeling\"), frequent words (\"is\", \"it\", \"for\", \"to\"), words in small letters, a url, stopwords (\"this\", \"for\", \"to\"), whitespace issues, emoticons (ðŸ˜ƒ), and emojis (ðŸš€) and numbers 1,2,3,4. these impurities would need to be addressed through preprocessing steps to ensure the text is ready for further natural language processing tasks.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Removal of emojis"
      ],
      "metadata": {
        "id": "DW0w1mwX4voV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove emojis\n",
        "def remove_emojis(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "        \"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\",\n",
        "        flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "# Remove emojis\n",
        "input_str = remove_emojis(input_str)\n",
        "print(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QJwjDm74ym4",
        "outputId": "b97d777d-2c2d-40b3-b433-5cee71ffbb3b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" check out this amazing article on natural language processing (nlp) at  it covers everything from tokenization to sentiment analysis!  in today's digital age, we often encounter noisy and unstructured text data. to prepare it for analysis or modeling, we need to apply several preprocessing steps. let's consider an example sentence: 'i can't believe how excited i am about this ai model! ' this sentence contains uppercase letters, emojis, and frequent words like 'is', 'it', 'for', 'to'. these elements, including emoticons and the url ' must be cleaned or removed to facilitate accurate nlp processing.\"\n",
            "\n",
            "this paragraph deliberately includes uppercase letters, incorrect spellings (like \"processing\", \"modeling\"), frequent words (\"is\", \"it\", \"for\", \"to\"), words in small letters, a url, stopwords (\"this\", \"for\", \"to\"), whitespace issues, emoticons (), and emojis () and numbers 1,2,3,4. these impurities would need to be addressed through preprocessing steps to ensure the text is ready for further natural language processing tasks.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Removal of Numbers"
      ],
      "metadata": {
        "id": "Z7sHbTUy4_X4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = re.sub(r'\\d+', '', input_str)\n",
        "print(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui-Yplw14ynu",
        "outputId": "10495365-b4e9-4be0-a3bd-af1fa9351914"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" check out this amazing article on natural language processing (nlp) at  it covers everything from tokenization to sentiment analysis!  in today's digital age, we often encounter noisy and unstructured text data. to prepare it for analysis or modeling, we need to apply several preprocessing steps. let's consider an example sentence: 'i can't believe how excited i am about this ai model! ' this sentence contains uppercase letters, emojis, and frequent words like 'is', 'it', 'for', 'to'. these elements, including emoticons and the url ' must be cleaned or removed to facilitate accurate nlp processing.\"\n",
            "\n",
            "this paragraph deliberately includes uppercase letters, incorrect spellings (like \"processing\", \"modeling\"), frequent words (\"is\", \"it\", \"for\", \"to\"), words in small letters, a url, stopwords (\"this\", \"for\", \"to\"), whitespace issues, emoticons (), and emojis () and numbers ,,,. these impurities would need to be addressed through preprocessing steps to ensure the text is ready for further natural language processing tasks.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Removal of Punctuations"
      ],
      "metadata": {
        "id": "_Pdu5FTe5Gtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = re.sub(r'[^\\w\\s]','',input_str)\n",
        "print(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK51WTzp5Qja",
        "outputId": "e0aed4f0-2c14-47a6-efff-814b34ecf65b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " check out this amazing article on natural language processing nlp at  it covers everything from tokenization to sentiment analysis  in todays digital age we often encounter noisy and unstructured text data to prepare it for analysis or modeling we need to apply several preprocessing steps lets consider an example sentence i cant believe how excited i am about this ai model  this sentence contains uppercase letters emojis and frequent words like is it for to these elements including emoticons and the url  must be cleaned or removed to facilitate accurate nlp processing\n",
            "\n",
            "this paragraph deliberately includes uppercase letters incorrect spellings like processing modeling frequent words is it for to words in small letters a url stopwords this for to whitespace issues emoticons  and emojis  and numbers  these impurities would need to be addressed through preprocessing steps to ensure the text is ready for further natural language processing tasks  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Removal of Whitespace"
      ],
      "metadata": {
        "id": "NcPkWo0_5TLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = re.sub(r'\\s+', ' ', input_str).strip()\n",
        "print(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdHBQY4u5QkQ",
        "outputId": "90a8ff97-7064-4fdb-992a-32662128625d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check out this amazing article on natural language processing nlp at it covers everything from tokenization to sentiment analysis in todays digital age we often encounter noisy and unstructured text data to prepare it for analysis or modeling we need to apply several preprocessing steps lets consider an example sentence i cant believe how excited i am about this ai model this sentence contains uppercase letters emojis and frequent words like is it for to these elements including emoticons and the url must be cleaned or removed to facilitate accurate nlp processing this paragraph deliberately includes uppercase letters incorrect spellings like processing modeling frequent words is it for to words in small letters a url stopwords this for to whitespace issues emoticons and emojis and numbers these impurities would need to be addressed through preprocessing steps to ensure the text is ready for further natural language processing tasks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "toqev5zz5Zeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Tokenization"
      ],
      "metadata": {
        "id": "w-J2W4Dt536D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(input_str)\n",
        "print(\"\\nTokens:\\n\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1Xe5azG55eA",
        "outputId": "7f04ddeb-0cb0-4767-b7c3-793252b0707a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokens:\n",
            " ['check', 'out', 'this', 'amazing', 'article', 'on', 'natural', 'language', 'processing', 'nlp', 'at', 'it', 'covers', 'everything', 'from', 'tokenization', 'to', 'sentiment', 'analysis', 'in', 'todays', 'digital', 'age', 'we', 'often', 'encounter', 'noisy', 'and', 'unstructured', 'text', 'data', 'to', 'prepare', 'it', 'for', 'analysis', 'or', 'modeling', 'we', 'need', 'to', 'apply', 'several', 'preprocessing', 'steps', 'lets', 'consider', 'an', 'example', 'sentence', 'i', 'cant', 'believe', 'how', 'excited', 'i', 'am', 'about', 'this', 'ai', 'model', 'this', 'sentence', 'contains', 'uppercase', 'letters', 'emojis', 'and', 'frequent', 'words', 'like', 'is', 'it', 'for', 'to', 'these', 'elements', 'including', 'emoticons', 'and', 'the', 'url', 'must', 'be', 'cleaned', 'or', 'removed', 'to', 'facilitate', 'accurate', 'nlp', 'processing', 'this', 'paragraph', 'deliberately', 'includes', 'uppercase', 'letters', 'incorrect', 'spellings', 'like', 'processing', 'modeling', 'frequent', 'words', 'is', 'it', 'for', 'to', 'words', 'in', 'small', 'letters', 'a', 'url', 'stopwords', 'this', 'for', 'to', 'whitespace', 'issues', 'emoticons', 'and', 'emojis', 'and', 'numbers', 'these', 'impurities', 'would', 'need', 'to', 'be', 'addressed', 'through', 'preprocessing', 'steps', 'to', 'ensure', 'the', 'text', 'is', 'ready', 'for', 'further', 'natural', 'language', 'processing', 'tasks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Removal of Stopwords\n"
      ],
      "metadata": {
        "id": "27nhwPwA59Tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [word for word in tokens if word not in stop_words]\n",
        "print(\"\\nTokens without stopwords:\\n\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyUdK-nm5-Ht",
        "outputId": "0b776bc6-8c0b-4eb0-e40c-8a7f99145cbd"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokens without stopwords:\n",
            " ['check', 'amazing', 'article', 'natural', 'language', 'processing', 'nlp', 'covers', 'everything', 'tokenization', 'sentiment', 'analysis', 'todays', 'digital', 'age', 'often', 'encounter', 'noisy', 'unstructured', 'text', 'data', 'prepare', 'analysis', 'modeling', 'need', 'apply', 'several', 'preprocessing', 'steps', 'lets', 'consider', 'example', 'sentence', 'cant', 'believe', 'excited', 'ai', 'model', 'sentence', 'contains', 'uppercase', 'letters', 'emojis', 'frequent', 'words', 'like', 'elements', 'including', 'emoticons', 'url', 'must', 'cleaned', 'removed', 'facilitate', 'accurate', 'nlp', 'processing', 'paragraph', 'deliberately', 'includes', 'uppercase', 'letters', 'incorrect', 'spellings', 'like', 'processing', 'modeling', 'frequent', 'words', 'words', 'small', 'letters', 'url', 'stopwords', 'whitespace', 'issues', 'emoticons', 'emojis', 'numbers', 'impurities', 'would', 'need', 'addressed', 'preprocessing', 'steps', 'ensure', 'text', 'ready', 'natural', 'language', 'processing', 'tasks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Removal of short tokens\n",
        "   \n"
      ],
      "metadata": {
        "id": "OhdcBnYd7kDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [word for word in tokens if len(word) > 3]\n",
        "print(\"\\nTokens without short tokens:\\n\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79pvGxLV7whH",
        "outputId": "6ec90a40-7b7c-4ce3-b884-9e041dd95bf7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokens without short tokens:\n",
            " ['check', 'amazing', 'article', 'natural', 'language', 'processing', 'covers', 'everything', 'tokenization', 'sentiment', 'analysis', 'todays', 'digital', 'often', 'encounter', 'noisy', 'unstructured', 'text', 'data', 'prepare', 'analysis', 'modeling', 'need', 'apply', 'several', 'preprocessing', 'steps', 'lets', 'consider', 'example', 'sentence', 'cant', 'believe', 'excited', 'model', 'sentence', 'contains', 'uppercase', 'letters', 'emojis', 'frequent', 'words', 'like', 'elements', 'including', 'emoticons', 'must', 'cleaned', 'removed', 'facilitate', 'accurate', 'processing', 'paragraph', 'deliberately', 'includes', 'uppercase', 'letters', 'incorrect', 'spellings', 'like', 'processing', 'modeling', 'frequent', 'words', 'words', 'small', 'letters', 'stopwords', 'whitespace', 'issues', 'emoticons', 'emojis', 'numbers', 'impurities', 'would', 'need', 'addressed', 'preprocessing', 'steps', 'ensure', 'text', 'ready', 'natural', 'language', 'processing', 'tasks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Spelling Check"
      ],
      "metadata": {
        "id": "naTONFRa6vPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_spelling(word):\n",
        "    corrected_word = str(TextBlob(word).correct())\n",
        "    return corrected_word\n",
        "\n",
        "tokens = [correct_spelling(word) for word in tokens]\n",
        "print(\"\\nTokens after spelling correction:\\n\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwv9m0IY66QP",
        "outputId": "515212d3-f9dd-4bd7-a868-77a6727e4504"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokens after spelling correction:\n",
            " ['check', 'amazing', 'article', 'natural', 'language', 'processing', 'covers', 'everything', 'tokenization', 'sentiment', 'analysis', 'today', 'digital', 'often', 'encounter', 'noisy', 'unstructured', 'text', 'data', 'prepare', 'analysis', 'modeling', 'need', 'apply', 'several', 'preprocessing', 'steps', 'lets', 'consider', 'example', 'sentence', 'can', 'believe', 'excited', 'model', 'sentence', 'contains', 'uppercase', 'letters', 'femoris', 'frequent', 'words', 'like', 'elements', 'including', 'emotions', 'must', 'cleaned', 'removed', 'facilitate', 'accurate', 'processing', 'paragraph', 'deliberately', 'includes', 'uppercase', 'letters', 'incorrect', 'swellings', 'like', 'processing', 'modeling', 'frequent', 'words', 'words', 'small', 'letters', 'stopford', 'whitespace', 'issues', 'emotions', 'femoris', 'numbers', 'immunities', 'would', 'need', 'addressed', 'preprocessing', 'steps', 'ensure', 'text', 'ready', 'natural', 'language', 'processing', 'tasks']\n"
          ]
        }
      ]
    }
  ]
}